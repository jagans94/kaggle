{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d4c08f48-fe23-4ddb-ac46-d97f05397514",
    "_uuid": "f2156d1dd26a1243e18512002e10872c5bd7271e"
   },
   "source": [
    "# Dog Breed Identification\n",
    "\n",
    "**Overview:**\n",
    "- Summarizing 'What I've learnt'?\n",
    "- Prepare Data\n",
    "- Inspect Data\n",
    "- Building, Training and Evaluating Models\n",
    "- Submission\n",
    "- Notes and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What I've learnt?\n",
    "- Concepts:\n",
    "- Code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "f67b9393-8ea1-4e23-b856-2ce149cfe421",
    "_execution_state": "idle",
    "_uuid": "72334cb006d02a4bcfc2a2fe622524eba824c6f8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jagan\\Anaconda3\\envs\\deep-learning\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, Conv2D, MaxPool2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.applications import VGG16\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# custom imports\n",
    "from scripts import my_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir=\"C:\\\\Users\\Jagan\\.kaggle\\competitions\\dog-breed-identification\"\n",
    "base_dir = '..\\datasets\\dog_breed_identification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed.\n",
      "Extracting from 'C:\\Users\\Jagan\\.kaggle\\competitions\\dog-breed-identification' to '..\\datasets\\dog_breed_identification'.\n",
      "\n",
      "This might take a while...\n",
      "Finished. Time taken: 41.29s.\n"
     ]
    }
   ],
   "source": [
    "my_utils.unpack_dataset(src_dir, base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed.\n",
      "Total no. of files: \n",
      "Selected no. of files:  10222\n",
      "\n",
      "This might take a while.... No. of train samples: 8177\n",
      "No. of validation samples: 2045\n",
      "Done. Time taken: 120.49s.\n"
     ]
    }
   ],
   "source": [
    "mapping = pd.read_csv(os.path.join(base_dir,'labels.csv'))\n",
    "train_dir, val_dir = my_utils.bin_dataset(base_dir, 'train', mapping.values, validation_split = 0.2)\n",
    "test_dir = os.path.join( base_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to save models\n",
    "save_dir = '..\\saved_models\\dog_breed_identification'\n",
    "if not os.path.isdir(save_dir): os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Parameters:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols, img_chnls = 150, 150, 3\n",
    "input_shape = (img_rows,img_cols,img_chnls)\n",
    "target_size = input_shape[:-1]\n",
    "\n",
    "num_classes = np.unique(mapping.values[:,1]).shape[0]\n",
    "\n",
    "train_sample_count = 8177 \n",
    "val_sample_count = 2045\n",
    "test_sample_count = len(os.listdir(test_dir))\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d5265777-aeb3-449d-b171-d88cad74c0a4",
    "_uuid": "5fa18b37a9acd9e098bac1d12264b0dd4310fdd3"
   },
   "source": [
    "### Building, Training and Evaluating Models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Basic Model:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(optimizer = RMSprop(lr = 1e-4), \n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generators\n",
    "train_generator = my_utils.data_generator(train_dir, target_size, batch_size)\n",
    "val_generator = my_utils.data_generator(val_dir, target_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting parameters\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "history = model.fit_generator(\n",
    "                  train_generator,\n",
    "                  steps_per_epoch = train_sample_count // batch_size,\n",
    "                  epochs = epochs,\n",
    "                  validation_data = val_generator,\n",
    "                  validation_steps = val_sample_count // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model_name = 'dbi_basic_1.h5'\n",
    "model.save(os.path.join(save_dir, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model history\n",
    "my_utils.plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Basic Model with Data Augmentation:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(optimizer = RMSprop(lr = 1e-4), \n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generators\n",
    "train_generator = my_utils.data_generator(train_dir, target_size, batch_size, augment = True)\n",
    "val_generator = my_utils.data_generator(val_dir, target_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting parameters\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "history = model.fit_generator(\n",
    "                  train_generator,\n",
    "                  steps_per_epoch = train_sample_count // batch_size,\n",
    "                  epochs = epochs,\n",
    "                  validation_data = val_generator,\n",
    "                  validation_steps = val_sample_count // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save moodel\n",
    "model_name = 'dbi_basic_data_aug_2.h5'\n",
    "model.save(os.path.join(save_dir, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model history\n",
    "my_utils.plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating on test data\n",
    "test_generator = my_utils.data_generator(test_dir, target_size, batch_size)\n",
    "predictions = model.predict_generator(test_generator, \n",
    "                                      steps=None, \n",
    "                                      max_queue_size=10, \n",
    "                                      workers=1, \n",
    "                                      use_multiprocessing=False, \n",
    "                                      verbose=0)\n",
    "\n",
    "predictions = np.argmax(predictions, axis=-1) #multiple categories\n",
    "\n",
    "label_map = (train_generator.class_indices)\n",
    "label_map = dict((v,k) for k,v in label_map.items()) #flip k,v\n",
    "predictions = [label_map[k] for k in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Using Pre-Trained ConvNet:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg16 convbase\n",
    "vgg16_conv_base = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "vgg16_conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on there's two ways to go about this:\n",
    "- **Fast Feature Extraction:** Running the convolutional base over our dataset, recording its output to a Numpy array on disk, then using this data as input to a standalone densely-connected classifier. This solution is very fast and cheap to run, because it only requires running the convolutional base once for every input image, and the convolutional base is by far the most expensive part of the pipeline. However, for the exact same reason, this technique would not allow us to leverage data augmentation at all.\n",
    "- **Model Extension:** Extending the model we have (conv_base) by adding Dense layers on top, and running the whole thing end-to-end on the input data. This allows us to use data augmentation, because every input image is going through the convolutional base every time it is seen by the model. However, for this same reason, this technique is far more expensive than the first one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fast Feature Extraction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shape for dense model\n",
    "feature_shape = (vgg16_conv_base.layers[-1]).output_shape\n",
    "dm_input_shape = np.prod(feature_shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense model\n",
    "model = Sequential()\n",
    "    \n",
    "model.add(Dense(512, activation='relu', input_shape=input_shape))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "dense_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile modle\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generators\n",
    "train_generator = my_utils.data_generator(train_dir, target_size, batch_size)\n",
    "val_generator = my_utils.data_generator(val_dir, target_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features\n",
    "train_features, train_labels = my_utils.extract_features(vgg16_conv_base, train_sample_count, \n",
    "                                                           train_generator, num_classes, batch_size)\n",
    "val_features, val_labels = my_utils.extract_features(vgg16_conv_base, val_sample_count, \n",
    "                                                       val_generator, num_classes, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape extracted features  \n",
    "train_features = train_features.reshape(train_sample_count, -1)\n",
    "val_features = val_features.reshape(val_sample_count, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting parameters\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "history = model.fit(train_features, train_labels,\n",
    "                    epochs = epochs,\n",
    "                    batch_size = batch_size,\n",
    "                    validation_data = (validation_features, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save moodel\n",
    "model_name = 'dbi_pre_trained_fast_feature_extraction_3.h5'\n",
    "model.save(os.path.join(save_dir, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model history\n",
    "my_utils.plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Extension:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(vgg16_conv_base)\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze the conv_base\n",
    "print('This is the number of trainable weights before freezing the conv base:', len(model.trainable_weights))\n",
    "vgg16_conv_base.trainable = False\n",
    "print('This is the number of trainable weights after freezing the conv base:', len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile modle\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generators\n",
    "train_generator = my_utils.data_generator(train_dir, target_size, batch_size, augment = True)\n",
    "val_generator = my_utils.data_generator(val_dir, target_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting parameters\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "history = model.fit_generator(\n",
    "                  train_generator,\n",
    "                  steps_per_epoch = train_sample_count // batch_size,\n",
    "                  epochs = epochs,\n",
    "                  validation_data = val_generator,\n",
    "                  validation_steps = val_sample_count // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save moodel\n",
    "model_name = 'dbi_pre_trained_model_extension_4.h5'\n",
    "model.save(os.path.join(save_dir, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model history\n",
    "my_utils.plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fine-tuning:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freezing everything but the last block\n",
    "conv_base.trainable = True\n",
    "set_trainable = False\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name == 'block5_conv1':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(lr=1e-5),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting parameters\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "history = model.fit_generator(\n",
    "                  train_generator,\n",
    "                  steps_per_epoch = train_sample_count // batch_size,\n",
    "                  epochs = epochs,\n",
    "                  validation_data = val_generator,\n",
    "                  validation_steps = val_sample_count // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save moodel\n",
    "model_name = 'dbi_fine_tuning_5.h5'\n",
    "model.save(os.path.join(save_dir, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model history\n",
    "my_utils.plot_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
